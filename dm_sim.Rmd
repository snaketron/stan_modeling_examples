---
title: "Working with count compositional data"
author: "SK"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: 
    toc: true
    toc_depth: 2
  code_folding: hide
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = FALSE)
```

```{r, eval=TRUE, echo=F, message=F, warning=F, comment=F}
library(rstan, quietly=T, verbose = F)
rstan_options(auto_write = TRUE)
library(ggplot2)
require(patchwork)
```


# Introduction
Dirichlet multinomial (DM) distribution is a compound probability distribution,
where a probability vector $\theta$ is drawn from a Dirichlet distribution with
parameter vector $\alpha$ and an observation data (counts; $y$) drawn from a 
multinomial distribution with probability vector $\theta$ and total number of 
trials $n$. The Dirichlet prior - the vector $\alpha\in\mathbb{R}^{+}$, can be 
seen as **pseudo**count:
  
  * $y \sim \mathrm{Multinomial}(\theta, n)$
  * $\theta \sim \mathrm{Dirichlet}(\alpha)$
  * $\alpha = \ldots$

Crash course on Dirichlet and Multinomial distributions: 
https://ixkael.github.io/dirichlet-multinomial-conjugate-priors/


# Lets simulate data from DM distribution 
Here I assume we have samples obtained from a number of patients before 
and after treatment with a drug. Each sample measures the mRNA abundances 
(**counts**) of $K$ genes (categories). Hence, sample $i$ is represented by 
K-vector $\vec{y}_{i}=\{y_{i1},y_{i2},\ldots,y_{ik}\}$. 

For patient $i$ we have two such samples: 

  * $\vec{y}_i^{b}$: collected before treatment
  * $\vec{y}_i^{a}$: collected after treatment

The sum of mRNAs observed in each sample is fixed to $n_i=10^4$, i.e., 
$n_i=\sum_{j=1}^k y_{ij}=10^4$. We can use the STAN model shown below 
to simulate data from DM distribution:

  * $y \sim \mathrm{Multinomial}(\theta, n)$
  * $\theta \sim \mathrm{Dirichlet}(\alpha)$
  * $\alpha = \xi\cdot \mathrm{softmax}(\gamma + \beta x)$ 

where $x$ is a design variable, with $x=1$ for samples before and $x=-1$ 
for samples after treatment. $\gamma$ is an intercept, $\beta$ represents 
the effect of the treatment, and $\xi\in\mathbb{R}^{+}$ is precision.

```{stan, output.var = "model_sim"}
data {
  int<lower=0> K; // categories (genes)
  vector[K] gamma; // intercept
  vector[K] beta; // effect
  real xi; // precision
  int n; // total counts (tries)
}

generated quantities {
  int y_a [K]; // sample after
  int y_b [K]; // sample before
  vector [K] alpha [2]; // intermediate alphas from dirichlet dist.
  alpha[1] = dirichlet_rng(xi*softmax(gamma + beta)); // before treatment sim. from DM.
  alpha[2] = dirichlet_rng(xi*softmax(gamma - beta)); // after treatment sim. from DM.
  y_a = multinomial_rng(alpha[1], n); // simulate from multinomial dist. -> after sample
  y_b = multinomial_rng(alpha[2], n); // simulate from multinomial dist. -> before sample 
}
```

Now we only need to provide the inputs (see data block in the STAN model) to 
simulate data 

```{r}
set.seed(seed = 12345)
K <- 20
gamma <- rnorm(n = K, mean = 0, sd = 1)
beta <- rnorm(n = K, mean = 0, sd = 0.25)
xi <- 100
n <- 10^4
```

... and simulate 20 samples with:

```{r}
sim_data <- rstan::sampling(
  object = model_sim,
  data = list(K = K, gamma = gamma, beta = beta, xi = xi, n = n),
  chain = 1,
  cores = 1,
  iter = 20,
  warmup = 0,
  algorithm = "Fixed_param",
  seed = 12345,
  verbose = FALSE,
  refresh = -1)
```

We then extract the simulated observations: two matrices $\vec{y}^{a}$ and 
$\vec{y}^{b}$, with rows as samples and columns as genes. We treat the first 10 
samples (rows in each matrix) as observed/training samples ($D_{\text{train}}$), 
and we will treat the remaining 10 samples as unobserved/testing sample 
($D_{\text{test}}$).

```{r, echo=FALSE}
# extract simulated data
y_b <- rstan::extract(sim_data, par = "y_b")$y_b
y_a <- rstan::extract(sim_data, par = "y_a")$y_a

# D_train: observed data
y_b_o <- data.frame(reshape::melt(y_b[1:10, ]))
y_a_o <- data.frame(reshape::melt(y_a[1:10, ]))

# D_test: test data
y_b_t <- data.frame(reshape::melt(y_b[11:20, ]))
y_a_t <- data.frame(reshape::melt(y_a[11:20, ]))
```


```{r, echo = FALSE}
# format D_train
colnames(y_b_o) <- c("sample_id", "gene_name", "gene_usage_count")
y_b_o$condition <- "before"

colnames(y_a_o) <- c("sample_id", "gene_name", "gene_usage_count")
y_a_o$condition <- "after"

u <- rbind(y_b_o, y_a_o)
rm(y_a_o, y_b_o)
```


```{r, echo = FALSE}
# format  D_test
colnames(y_b_t) <- c("sample_id", "gene_name", "gene_usage_count")
y_b_t$condition <- "before"

colnames(y_a_t) <- c("sample_id", "gene_name", "gene_usage_count")
y_a_t$condition <- "after"

t <- rbind(y_b_t, y_a_t)
rm(y_a_t, y_b_t)
```


```{r, echo = FALSE}
# We will use IgGeneUsage’s utility function to structure ‘u’ into an 
# object that will make analysis convenient


get_paired_usage <- function(u) {
  u_t <- u[base::duplicated(u[, c("sample_id", "condition")])==FALSE,]
  u_t <- base::table(u_t$condition, u_t$sample_id)
  if(base::any(u_t!=1)) {
    stop(paste0("sample/s: ", 
                paste0(names(which(colSums(u_t)!=2)), collapse = ','),
                " not paired."))
  }
  
  # get Y data, fill empty combinations with 0
  Y <- reshape2::acast(data = u, 
                       formula = gene_name~sample_id+condition,
                       drop = FALSE, 
                       value.var = "gene_usage_count",
                       fill = 0, 
                       fun.aggregate = base::sum)
  Y <- reshape2::melt(Y)
  
  # processed data
  colnames(Y) <- c("gene_name", "sc", "gene_usage_count")
  k <- do.call(rbind, strsplit(x = as.character(Y$sc), split = '\\_'))
  Y$sample_id <- k[,1]
  Y$condition <- k[,2]
  Y$sc <- NULL
  
  # compute total usage
  N <- stats::aggregate(gene_usage_count~sample_id+condition, 
                        data = Y,
                        FUN = base::sum, 
                        drop = FALSE)
  N$total_usage_count <- N$gene_usage_count
  N$gene_usage_count <- NULL
  
  # merge usage and total usage
  Y <- base::merge(x = Y, y = N, 
                   by = c("sample_id", "condition"),
                   all.x = T)
  Y$gene_usage_prop <- Y$gene_usage_count/Y$total_usage_count
  Y$gene_name <- base::as.character(Y$gene_name)
  
  cs <- base::sort(base::unique(Y$condition))
  
  # get usage matrices
  Y_1 <- reshape2::acast(data = Y[Y$condition == cs[1], ], 
                         formula = gene_name~sample_id,
                         drop = FALSE, 
                         value.var = "gene_usage_count",
                         fill = 0, 
                         fun.aggregate = base::sum)
  Y_2 <- reshape2::acast(data = Y[Y$condition == cs[2], ], 
                         formula = gene_name~sample_id,
                         drop = FALSE, 
                         value.var = "gene_usage_count",
                         fill = 0, 
                         fun.aggregate = base::sum)
  
  if(ncol(Y_1)==1) {
    r_col <- base::colnames(Y_1)
    Y_1 <- Y_1[base::sort(base::rownames(Y_1)),]
    r <- matrix(data = Y_1, nrow = length(Y_1), ncol = 1)
    rownames(r) <- names(Y_1)
    colnames(r) <- r_col
    Y_1 <- r
    rm(r, r_col)
  } 
  else {
    Y_1 <- Y_1[base::sort(base::rownames(Y_1)), 
               base::sort(base::colnames(Y_1))]
  }
  if(ncol(Y_2)==1) {
    r_col <- base::colnames(Y_2)
    Y_2 <- Y_2[base::sort(base::rownames(Y_2)),]
    r <- matrix(data = Y_2, nrow = length(Y_2), ncol = 1)
    rownames(r) <- names(Y_2)
    colnames(r) <- r_col
    Y_2 <- r
    rm(r, r_col)
  } 
  else {
    Y_2 <- Y_2[base::sort(base::rownames(Y_2)), 
               base::sort(base::colnames(Y_2))]
  }
  
  N <- base::cbind(base::colSums(Y_1), 
                   base::colSums(Y_2))
  
  return (base::list(Y_1 = Y_1, 
                     Y_2 = Y_2, 
                     N = N, 
                     N_sample = base::ncol(Y_1), 
                     N_gene = base::nrow(Y_1), 
                     gene_names = base::rownames(Y_1),
                     sample_names = base::colnames(Y_1), 
                     proc_ud = Y))
}


u <- get_paired_usage(u = u)
t <- get_paired_usage(u = t)

# add D_test data to object u
u$Y_1_test <- t$Y_1
u$Y_2_test <- t$Y_2
u$N_test <- t$N
u$N_sample_test <- t$N_sample
u$proc_ud_test <- t$proc_ud
rm(t)

# save for later use
save(u, file = "gene_usage.RData", compress = T)
```


```{r, echo=FALSE, eval=TRUE}
u <- get(load(file = "dm_sim/gene_usage.RData"))
```


```{r, echo = FALSE}
str(u)
```



We can visualize the data using ggplot. Each point is the mRNA count (y-axis)
for a given gene (x-axis) of a sample (point). Notice the differences in 
mRNA counts between the two conditions for genes 12, 5, 7, etc. Filled points
are train and hollow points are test data.

```{r, fig.width=6, fig.height=3.5, fig.align='center', eval=TRUE}
ggplot()+
  geom_point(data = u$proc_ud, aes(x = gene_name, y = gene_usage_count, col = condition),
             position = position_dodge(width = 0.5), size = 1)+
  geom_point(data = u$proc_ud_test, aes(x = gene_name, y = gene_usage_count, col = condition),
             position = position_dodge(width = 0.5), size = 1, shape = 21)+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  scale_color_manual(values = c("steelblue", "orange"))
```

## Modeling
We know that the data was generated from $DM$ distribution. What would happen 
if we assumed that the data comes from a) multinomial distribution (model $M$) 
and b) dirichlet multinomial distribution (model $DM$)?

For this we will use two models that I have developed for differential 
immunoglobulin gene usage (DGU) analysis between pairs of samples. The models
have *similar* structure as the model used for data simulation.

The main difference between the simulation model and the DGU models is that 
the DGU models are hierarchical. They assume that samples within a condition 
(here condition = "before" vs "after" treatment) are not completely independent
from each other, and use partial pooling to enable sharing of information 
(about parameters) between samples within a condition.

Model $DM$ and $M$ are identical, except for the following differences:

### Model $DM$

  * $\vec{y_i} \sim \mathrm{Multinomial}(\vec{\theta_i}, n_i)$
  * $\vec{\theta_i} \sim \mathrm{Dirichlet}(\xi \cdot \mathrm{softmax}(\vec{\mu_i}))$
  * $\vec{\mu_i} = \vec{\gamma}_i+\vec{\beta}_{i}x$

### Model $M$
  
  * $\vec{y_i} \sim \mathrm{Multinomial}(\vec{\theta_i}, n_i)$
  * $\vec{\theta_i} = \mathrm{softmax}(\vec{\mu_i})$
  * $\vec{\mu_i} = \vec{\gamma}_i+\vec{\beta}_{i}x$

### STAN code for model $DM$

```{stan, output.var = "model_dm"}
functions {
  real dirichlet_multinomial_complete_lpmf(int[] y, vector alpha) {
    real sum_alpha = sum(alpha);
    return lgamma(sum_alpha) - lgamma(sum(y) + sum_alpha)
           + lgamma(sum(y)+1) - sum(lgamma(to_vector(y)+1))
           + sum(lgamma(to_vector(y) + alpha)) - sum(lgamma(alpha));
  }
}

data {
  int <lower = 0> N_sample; // number of samples (repertoires)
  int <lower = 0> N_gene; // number of genes
  int Y_1 [N_gene, N_sample]; // number of successes (cells) in samples x gene
  int Y_2 [N_gene, N_sample]; // number of successes (cells) in samples x gene
  int N [N_sample, 2]; // number of total tries (repertoire size)
  // test data
  int <lower = 0> N_sample_test; // number of samples (repertoires)
  int Y_1_test [N_gene, N_sample_test]; // number of successes (cells) in samples x gene
  int Y_2_test [N_gene, N_sample_test]; // number of successes (cells) in samples x gene
  int N_test [N_sample_test, 2]; // number of total tries (repertoire size)
}

transformed data {
  real N_real [N_sample, 2];
  N_real = N;
}

parameters {
  vector [N_gene] alpha_mu_gene;
  real <lower=0> beta_sigma_gene;
  real <lower=0> alpha_sigma_gene;
  real <lower=0> beta_sigma_pop;
  vector [N_gene] beta_z [N_sample];
  vector [N_gene] alpha_z [N_sample];
  vector [N_gene] beta_z_gene;
  real <lower=0> xi;
}

transformed parameters {
  vector [N_gene] alpha [N_sample];
  vector [N_gene] beta [N_sample];
  vector [N_gene] beta_mu_gene;
  
  beta_mu_gene = 0+beta_sigma_pop*beta_z_gene;
  for(i in 1:N_sample) {
    beta[i] = beta_mu_gene + beta_sigma_gene * beta_z[i];
    alpha[i] = alpha_mu_gene + alpha_sigma_gene * alpha_z[i];
  }
}

model {
  target += exponential_lpdf(xi | 0.05);
  target += cauchy_lpdf(beta_sigma_pop | 0, 1);
  target += cauchy_lpdf(alpha_sigma_gene | 0, 1);
  target += cauchy_lpdf(beta_sigma_gene | 0, 1);
  for(i in 1:N_sample) {
    target += normal_lpdf(alpha_z[i] | 0, 1);
    target += normal_lpdf(beta_z[i] | 0, 1);
  }
  target += normal_lpdf(beta_z_gene | 0, 1);
  target += normal_lpdf(alpha_mu_gene | 0, 5);
  
  // likelihood
  for(i in 1:N_sample) {
    target += dirichlet_multinomial_complete_lpmf(Y_1[,i]|xi * softmax(alpha[i]-beta[i]));
    target += dirichlet_multinomial_complete_lpmf(Y_2[,i]|xi * softmax(alpha[i]+beta[i]));
  }
}

generated quantities {
  int Y_hat_1 [N_gene, N_sample];
  int Y_hat_2 [N_gene, N_sample];
  int Y_hat_group_1 [N_gene, N_sample];
  int Y_hat_group_2 [N_gene, N_sample];
  real log_lik [N_sample, 2];
  real log_lik_train [N_sample, 2];
  real log_lik_test [N_sample_test, 2];
  
  vector [N_gene] mu [2];
  real a [N_gene];
  real b [N_gene];

  for(i in 1:N_sample) {
    mu[1] = dirichlet_rng(xi * softmax(alpha[i]-beta[i]));
    Y_hat_1[,i] = multinomial_rng(mu[1], N[i,1]);
    mu[2] = dirichlet_rng(xi * softmax(alpha[i]+beta[i]));
    Y_hat_2[,i] = multinomial_rng(mu[2], N[i,2]);
    
    log_lik[i,1] = dirichlet_multinomial_complete_lpmf(Y_1[,i]|xi*softmax(alpha[i]-beta[i]));
    log_lik[i,2] = dirichlet_multinomial_complete_lpmf(Y_2[,i]|xi*softmax(alpha[i]+beta[i]));
  }
  
  // PPC: condition-specific
  a = normal_rng(alpha_mu_gene, alpha_sigma_gene);
  b = normal_rng(beta_mu_gene, beta_sigma_gene);
  mu[1] = xi * softmax(to_vector(a)-to_vector(b));
  mu[2] = xi * softmax(to_vector(a)+to_vector(b));
  // PPC: test
  for(i in 1:N_sample_test) {
    log_lik_test[i,1] = dirichlet_multinomial_complete_lpmf(Y_1_test[,i]|mu[1]);
    log_lik_test[i,2] = dirichlet_multinomial_complete_lpmf(Y_2_test[,i]|mu[2]);
  }
  // PPC: train
  for(i in 1:N_sample) {
    log_lik_train[i,1] = dirichlet_multinomial_complete_lpmf(Y_1[,i]|mu[1]);
    log_lik_train[i,2] = dirichlet_multinomial_complete_lpmf(Y_2[,i]|mu[2]);
    Y_hat_group_1[,i] = multinomial_rng(dirichlet_rng(mu[1]), N[i,1]);
    Y_hat_group_2[,i] = multinomial_rng(dirichlet_rng(mu[2]), N[i,2]);
  }
}
```


### STAN code for model $M$

```{stan, output.var = "model_m"}
data {
  int <lower = 0> N_sample; // number of samples (repertoires)
  int <lower = 0> N_gene; // number of genes
  int Y_1 [N_gene, N_sample]; // number of successes (cells) in samples x gene
  int Y_2 [N_gene, N_sample]; // number of successes (cells) in samples x gene
  int N [N_sample, 2]; // number of total tries (repertoire size)
  // test data
  int <lower = 0> N_sample_test; // number of samples (repertoires)
  int Y_1_test [N_gene, N_sample_test]; // number of successes (cells) in samples x gene
  int Y_2_test [N_gene, N_sample_test]; // number of successes (cells) in samples x gene
  int N_test [N_sample_test, 2]; // number of total tries (repertoire size)
}

transformed data {
  real N_real [N_sample, 2];
  N_real = N;
}

parameters {
  vector [N_gene] alpha_mu_gene;
  real <lower=0> beta_sigma_gene;
  real <lower=0> alpha_sigma_gene;
  real <lower=0> beta_sigma_pop;
  vector [N_gene] beta_z [N_sample];
  vector [N_gene] alpha_z [N_sample];
  vector [N_gene] beta_z_gene;
}

transformed parameters {
  vector [N_gene] alpha [N_sample];
  vector [N_gene] beta [N_sample];
  vector [N_gene] beta_mu_gene;
  
  beta_mu_gene = 0+beta_sigma_pop*beta_z_gene;
  for(i in 1:N_sample) {
    beta[i]=beta_mu_gene+beta_sigma_gene*beta_z[i];
    alpha[i]=alpha_mu_gene+alpha_sigma_gene*alpha_z[i];
  }
}

model {
  target += cauchy_lpdf(beta_sigma_pop | 0, 1);
  target += cauchy_lpdf(alpha_sigma_gene | 0, 1);
  target += cauchy_lpdf(beta_sigma_gene | 0, 1);
  for(i in 1:N_sample) {
    target += normal_lpdf(alpha_z[i] | 0, 1);
    target += normal_lpdf(beta_z[i] | 0, 1);
  }
  target += normal_lpdf(beta_z_gene | 0, 1);
  target += normal_lpdf(alpha_mu_gene | 0, 5);
  
  // likelihood
  for(i in 1:N_sample) {
    target += multinomial_lpmf(Y_1[,i] | softmax(alpha[i]-beta[i]));
    target += multinomial_lpmf(Y_2[,i] | softmax(alpha[i]+beta[i]));
  }
}

generated quantities {
  int Y_hat_1 [N_gene, N_sample];
  int Y_hat_2 [N_gene, N_sample];
  int Y_hat_group_1 [N_gene, N_sample];
  int Y_hat_group_2 [N_gene, N_sample];
  real log_lik [N_sample, 2];
  real log_lik_test [N_sample_test, 2];
  real log_lik_train [N_sample, 2];
  
  vector [N_gene] mu [2];
  real a [N_gene];
  real b [N_gene];
  
  // PPC
  for(i in 1:N_sample) {
    mu[1] = softmax(alpha[i] - beta[i]);
    mu[2] = softmax(alpha[i] + beta[i]);
    
    Y_hat_1[,i] = multinomial_rng(mu[1], sum(Y_1[,i]));
    Y_hat_2[,i] = multinomial_rng(mu[2], sum(Y_2[,i]));
    log_lik[i,1] = multinomial_lpmf(Y_1[,i] | mu[1]);
    log_lik[i,2] = multinomial_lpmf(Y_2[,i] | mu[2]);
  }
  
  // PPC - test (condition-specific)
  a = normal_rng(alpha_mu_gene, alpha_sigma_gene);
  b = normal_rng(beta_mu_gene, beta_sigma_gene);
  mu[1]=softmax(to_vector(a)-to_vector(b));
  mu[2]=softmax(to_vector(a)+to_vector(b));
  for(i in 1:N_sample_test) {
    log_lik_test[i,1] = multinomial_lpmf(Y_1_test[,i]|mu[1]);
    log_lik_test[i,2] = multinomial_lpmf(Y_2_test[,i]|mu[2]);
  }
  for(i in 1:N_sample) {
    log_lik_train[i,1] = multinomial_lpmf(Y_1[,i]|mu[1]);
    log_lik_train[i,2] = multinomial_lpmf(Y_2[,i]|mu[2]);
    Y_hat_group_1[,i] = multinomial_rng(mu[1], N[i,1]);
    Y_hat_group_2[,i] = multinomial_rng(mu[2], N[i,2]);
  }
}
```

### Model fitting
We will fit models $DM$ and $M$ using $D_{\text{train}}$.

```{r}
fit_dm <- rstan::sampling(object = model_dm,
                          data = u,
                          chains = 5,
                          cores = 5,
                          iter = 3500,
                          warmup = 1500,
                          control = list(adapt_delta = 0.99, 
                                         max_treedepth = 14),
                          algorithm = "NUTS")
```


```{r, echo = FALSE}
save(fit_dm, file = "dm_sim/fit_dm.RData", compress = T)
```


```{r}
fit_m <- rstan::sampling(object = model_m,
                         data = u,
                         chains = 5,
                         cores = 5,
                         iter = 3500,
                         warmup = 1500,
                         control = list(adapt_delta = 0.99, max_treedepth = 14),
                         algorithm = "NUTS")
```


```{r, echo = FALSE}
save(fit_m, file = "dm_sim/fit_m.RData")
```


```{r, echo=FALSE, eval = TRUE, echo=F}
fit_dm <- get(load(file = "dm_sim/fit_dm.RData"))
fit_m <- get(load(file = "dm_sim/fit_m.RData"))
```


## Posterior predictive checks for each sample
We will use the parameters at the lowest level of the models, which are sample
specific parameters, to generate new data. These predictions (means and 95\% 
HDIs) will be compared against the observed data. 

Please consider the narrow predictions by model $M$ compared to $DM$. 

```{r, eval = TRUE, echo = FALSE}

get_ppc <- function(fit, gene_usage, model_name) {
  summary_y_hat_1 <- data.frame(summary(fit, par = "Y_hat_1")$summary)
  summary_y_hat_1$Y <- as.vector(t(gene_usage$Y_1))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_1|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_1)),
                               split = ','))
  summary_y_hat_1$gene_name <- x[, 1]
  summary_y_hat_1$sample_id <- x[, 2]
  summary_y_hat_1$condition <- "A"
  
  summary_y_hat_2 <- data.frame(summary(fit, par = "Y_hat_2")$summary)
  summary_y_hat_2$Y <-as.vector(t(gene_usage$Y_2))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_2|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_2)),
                               split = ','))
  summary_y_hat_2$gene_name <- x[, 1]
  summary_y_hat_2$sample_id <- x[, 2]
  summary_y_hat_2$condition <- "B"
  
  s <- rbind(summary_y_hat_1, summary_y_hat_2)
  s$model_name <- model_name
  return(s)
}

```

```{r, eval = TRUE, echo = FALSE}
# DM
ppc_dm <- get_ppc(fit = fit_dm, gene_usage = u, model_name = "DM")

# M
ppc_m <- get_ppc(fit = fit_m, gene_usage = u, model_name = "M")

ppc <- rbind(ppc_dm, ppc_m)
rm(ppc_dm, ppc_m)
```

```{r, fig.width=6, fig.height=12, fig.align='center', eval=TRUE}
ggplot(data = ppc)+
  geom_abline(slope = 1, intercept = 0)+
  facet_grid(sample_id~model_name)+
  geom_point(aes(y = mean, x = Y, col = condition), size = 1)+
  geom_errorbar(aes(y = mean, x = Y, ymin = X2.5., ymax = X97.5., col = condition))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Observed Y")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))
```

Nearly all observations fall within the 95\% HDIs of the counts predicted by
either model.

```{r, eval = TRUE, echo = FALSE}
ppc$in_hdi <- ifelse(ppc$Y >= ppc$X2.5. & ppc$Y <= ppc$X97.5., 
                     yes = "in_hdi", no = "out_hdi")
table(ppc$in_hdi, ppc$model_name)
```


## PPC from condition-specific parameters
To check how well our models predicts new data ($D_{\text{test}}$), we cannot 
use the sample-specific parameters. For this, we have to use the condition-
specific parameters found at the 2nd layer of model structure (see STAN code; 
generated quantities).

Now, the model $M$ predictions (means and 95\% HDIs) are associated with large 
degree of uncertainty. However, many low count observations are outside the 
95\% HDIs of the predictions made by model $M$. Model $DM$ does a better 
prediction overall using its condition-specific parameters.

```{r, eval=TRUE, echo = FALSE}

get_ppc_group <- function(fit, gene_usage, model_name) {
  summary_y_hat_1 <- data.frame(summary(fit, par = "Y_hat_group_1")$summary)
  summary_y_hat_1$Y_Din <- as.vector(t(gene_usage$Y_1))
  summary_y_hat_1$Y_Dout <- as.vector(t(gene_usage$Y_1_test))
  
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_group_1|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_1)),
                               split = ','))
  summary_y_hat_1$gene_name <- x[, 1]
  summary_y_hat_1$sample_id <- x[, 2]
  summary_y_hat_1$condition <- "A"
  
  summary_y_hat_2 <- data.frame(summary(fit, par = "Y_hat_group_2")$summary)
  summary_y_hat_2$Y_Din <- as.vector(t(gene_usage$Y_2))
  summary_y_hat_2$Y_Dout <- as.vector(t(gene_usage$Y_2_test))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_group_2|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_2)),
                               split = ','))
  summary_y_hat_2$gene_name <- x[, 1]
  summary_y_hat_2$sample_id <- x[, 2]
  summary_y_hat_2$condition <- "B"
  
  s <- rbind(summary_y_hat_1, summary_y_hat_2)
  s$model_name <- model_name
  return(s)
}

```

```{r, eval=TRUE, echo = FALSE}
# DM
ppc_group_dm <- get_ppc_group(fit = fit_dm, gene_usage = u, model_name = "DM")

# M
ppc_group_m <- get_ppc_group(fit = fit_m, gene_usage = u, model_name = "M")

ppc_group <- rbind(ppc_group_dm, ppc_group_m)
rm(ppc_group_dm, ppc_group_m)
```

```{r, fig.width=12, fig.height=7, fig.align='center', eval=TRUE, echo = FALSE}
(ggplot(data = ppc_group)+
  facet_wrap(facets = ~model_name, ncol = 1)+
  geom_point(aes(x = gene_name, y = mean, col = condition), 
                position = position_dodge(width = 0.55),  size = 0.5)+
  geom_point(aes(x = gene_name, y = Y_Din, group = condition),
                position = position_dodge(width = 0.55), size = 1.1)+
  geom_errorbar(aes(x = gene_name, y = mean, ymin = X2.5., ymax = X97.5., col = condition), 
                width = 0.35, position = position_dodge(width = 0.55))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Gene")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))+
   ggtitle(label = "D_train"))|
(ggplot(data = ppc_group)+
  facet_wrap(facets = ~model_name, ncol = 1)+
  geom_point(aes(x = gene_name, y = mean, col = condition), 
                position = position_dodge(width = 0.55),  size = 0.5)+
  geom_point(aes(x = gene_name, y = Y_Dout, group = condition),
                position = position_dodge(width = 0.55), size = 1.1)+
  geom_errorbar(aes(x = gene_name, y = mean, ymin = X2.5., ymax = X97.5., col = condition), 
                width = 0.35, position = position_dodge(width = 0.55))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Gene")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))+
   ggtitle(label = "D_test"))
```


### How many $D_{\text{train}}$ observations are outside the 95\% HDIs of the predictions?
Model $DM$ does more meaningful predictions with respect to both 
$D_{\text{test}}$ and $D_{\text{train}}$.

```{r, eval=TRUE, echo = FALSE}
ppc_group$in_hdi_in <- ifelse(ppc_group$Y_Din >= ppc_group$X2.5. & 
                             ppc_group$Y_Din <= ppc_group$X97.5., 
                           yes = "in_hdi", no = "out_hdi")
table(ppc_group$in_hdi_in, ppc_group$model_name)
```

### How many $D_{\text{test}}$ observations are outside the 95\% HDIs of the predictions?

```{r, eval=TRUE, echo = FALSE}
ppc_group$in_hdi_out <- ifelse(ppc_group$Y_Dout >= ppc_group$X2.5. & 
                             ppc_group$Y_Dout <= ppc_group$X97.5., 
                           yes = "in_hdi", no = "out_hdi")
table(ppc_group$in_hdi_out, ppc_group$model_name)
```




## Model comparison with LOO
### with R-package loo (or rstan)
Now, lets turn to LOO with R-package loo. We get many high $k$ diagnostic 
values. We have too few samples per condition and this is why the approximation 
fails. Increasing the number of samples to 30 produced mostly "good" and few 
"ok" k diagnostic values. We should use explicit loo in case of few samples.

Nevertheless, due to the bad approximation we cannot rely on these results.

```{r, echo = FALSE, eval=T}
rstan::loo(fit_dm)
```


```{r, echo = FALSE, eval=T}
rstan::loo(fit_m)
```


## Model comparison with the Widely Applicable Information Criterion (WAIC)
We compute the average log probability ($\text{lppd}$) for observation $y_i$ 
given posterior draw $s$ of the model parameters ($\Theta$): $\text{lppd}(y,\Theta)=\sum\limits_{i=1}^n\text{log}\dfrac{1}{S}\sum\limits_{s=1}^S \text{Pr}(y_i|\Theta_s)$. Moreover, we compute the effective number of 
parameters, $p_{\text{eff}}=\sum\limits_{i=1}^n V(y_i)$, where $V(y_i)$ is the 
variance of the log probability of observation $y_i$. Finally, 
$\text{WAIC} = -2(\text{lppd}-p_{\text{eff}})$, and we compute $\text{WAIC}$ for 
$D_{\text{train}}$ and $D_{\text{test}}$ for each model.


### Model $DM$ WAIC

```{r, eval=TRUE, echo = FALSE}
loglik_train_dm <- summary(fit_dm, par = "log_lik_train")$summary
loglik_test_dm <- summary(fit_dm, par = "log_lik_test")$summary
```

#### $D_{\text{train}}$
Notice that the loo package generated similar estimates:

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_train_dm[,"mean"]), "\n")
cat("p_eff:", sum(loglik_train_dm[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_train_dm[,"mean"])-sum(loglik_train_dm[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_train_dm[,"mean"])*sqrt(length(loglik_train_dm[,"mean"])))
# see https://avehtari.github.io/modelselection/CV-FAQ.html#15_Why_(sqrt{n})_in_Standard_error_(SE)_of_LOO
```

#### $D_{\text{test}}$
As expected, we see slightly worse (larger) \text{WAIC} for model $DM$ for 
$D_{\text{test}}$ compared to $D_{\text{train}}$:

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_test_dm[,"mean"]), "\n")
cat("p_eff:", sum(loglik_test_dm[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_test_dm[,"mean"])-sum(loglik_test_dm[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_test_dm[,"mean"])*sqrt(length(loglik_test_dm[,"mean"])))
```


### Model $M$ WAIC
For model $M$, we got $\text{lppd}(D_{\text{train}})$ and 
$p_{\text{eff}}(D_{\text{train}})$ similar to those generated by the R-package 
loo. Notice that $\text{lppd}(D_{\text{train}})$ of model $M$ is much smaller 
than $\text{lppd}(D_{\text{train}})$ of model $DM$. This picture is consistent
with the accurate predictions of model $M$ with narrow 95\% HDIs.

```{r, eval=TRUE, echo = FALSE}
loglik_train_m <- summary(fit_m, par = "log_lik_train")$summary
loglik_test_m <- summary(fit_m, par = "log_lik_test")$summary
```

#### $D_{\text{train}}$

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_train_m[,"mean"]), "\n")
cat("p_eff:", sum(loglik_train_m[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_train_m[,"mean"])-sum(loglik_train_m[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_train_m[,"mean"])*sqrt(length(loglik_train_m[,"mean"])))
```

#### $D_{\text{test}}$
However, now have a look at the incredible high (bad) values for 
$\text{lppd}(D_{\text{test}})$ of model $M$ given $D_{\text{test}}$. We see 
especially high $p_{\text{eff}}$, which makes sense given the highly variable 
PPCs.

This indicates overfitting, i.e. model $M$ can predict accurately 
$D_{\text{train}}$ based on the sample-specific parameters, however, 
the model does not generalize well to new data. 

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_test_m[,"mean"]), "\n")
cat("p_eff:", sum(loglik_test_m[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_test_m[,"mean"])-sum(loglik_test_m[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_test_m[,"mean"])*sqrt(length(loglik_test_m[,"mean"])))
```



# Now the other way around: simulate data from $M$ distribution 

```{stan, output.var = "model_sim_m"}
data {
  int<lower=0> K; // categories (genes)
  vector[K] gamma; // intercept
  vector[K] beta; // effect
  int n; // total counts (tries)
}

generated quantities {
  int y_a [K]; // sample after
  int y_b [K]; // sample before
  y_a = multinomial_rng(softmax(gamma + beta), n); // before treatment sim. from M.
  y_b = multinomial_rng(softmax(gamma - beta), n); // after treatment sim. from M.
}
```

```{r}
set.seed(seed = 12345)
K <- 20
gamma <- rnorm(n = K, mean = 0, sd = 1)
beta <- rnorm(n = K, mean = 0, sd = 0.25)
n <- 10^4
```


```{r}
sim_data <- rstan::sampling(
  object = model_sim_m,
  data = list(K = K, gamma = gamma, beta = beta, n = n),
  chain = 1,
  cores = 1,
  iter = 20,
  warmup = 0,
  algorithm = "Fixed_param",
  seed = 12345,
  verbose = FALSE,
  refresh = -1)
```

```{r, echo=FALSE}
# extract simulated data
y_b <- rstan::extract(sim_data, par = "y_b")$y_b
y_a <- rstan::extract(sim_data, par = "y_a")$y_a

# D_train: observed data
y_b_o <- data.frame(reshape::melt(y_b[1:10, ]))
y_a_o <- data.frame(reshape::melt(y_a[1:10, ]))

# D_test: test data
y_b_t <- data.frame(reshape::melt(y_b[11:20, ]))
y_a_t <- data.frame(reshape::melt(y_a[11:20, ]))
```


```{r, echo = FALSE}
# format D_train
colnames(y_b_o) <- c("sample_id", "gene_name", "gene_usage_count")
y_b_o$condition <- "before"

colnames(y_a_o) <- c("sample_id", "gene_name", "gene_usage_count")
y_a_o$condition <- "after"

u <- rbind(y_b_o, y_a_o)
rm(y_a_o, y_b_o)
```


```{r, echo = FALSE}
# format  D_test
colnames(y_b_t) <- c("sample_id", "gene_name", "gene_usage_count")
y_b_t$condition <- "before"

colnames(y_a_t) <- c("sample_id", "gene_name", "gene_usage_count")
y_a_t$condition <- "after"

t <- rbind(y_b_t, y_a_t)
rm(y_a_t, y_b_t)
```


```{r, echo = FALSE}
# We will use IgGeneUsage’s utility function to structure ‘u’ into an 
# object that will make analysis convenient


get_paired_usage <- function(u) {
  u_t <- u[base::duplicated(u[, c("sample_id", "condition")])==FALSE,]
  u_t <- base::table(u_t$condition, u_t$sample_id)
  if(base::any(u_t!=1)) {
    stop(paste0("sample/s: ", 
                paste0(names(which(colSums(u_t)!=2)), collapse = ','),
                " not paired."))
  }
  
  # get Y data, fill empty combinations with 0
  Y <- reshape2::acast(data = u, 
                       formula = gene_name~sample_id+condition,
                       drop = FALSE, 
                       value.var = "gene_usage_count",
                       fill = 0, 
                       fun.aggregate = base::sum)
  Y <- reshape2::melt(Y)
  
  # processed data
  colnames(Y) <- c("gene_name", "sc", "gene_usage_count")
  k <- do.call(rbind, strsplit(x = as.character(Y$sc), split = '\\_'))
  Y$sample_id <- k[,1]
  Y$condition <- k[,2]
  Y$sc <- NULL
  
  # compute total usage
  N <- stats::aggregate(gene_usage_count~sample_id+condition, 
                        data = Y,
                        FUN = base::sum, 
                        drop = FALSE)
  N$total_usage_count <- N$gene_usage_count
  N$gene_usage_count <- NULL
  
  # merge usage and total usage
  Y <- base::merge(x = Y, y = N, 
                   by = c("sample_id", "condition"),
                   all.x = T)
  Y$gene_usage_prop <- Y$gene_usage_count/Y$total_usage_count
  Y$gene_name <- base::as.character(Y$gene_name)
  
  cs <- base::sort(base::unique(Y$condition))
  
  # get usage matrices
  Y_1 <- reshape2::acast(data = Y[Y$condition == cs[1], ], 
                         formula = gene_name~sample_id,
                         drop = FALSE, 
                         value.var = "gene_usage_count",
                         fill = 0, 
                         fun.aggregate = base::sum)
  Y_2 <- reshape2::acast(data = Y[Y$condition == cs[2], ], 
                         formula = gene_name~sample_id,
                         drop = FALSE, 
                         value.var = "gene_usage_count",
                         fill = 0, 
                         fun.aggregate = base::sum)
  
  if(ncol(Y_1)==1) {
    r_col <- base::colnames(Y_1)
    Y_1 <- Y_1[base::sort(base::rownames(Y_1)),]
    r <- matrix(data = Y_1, nrow = length(Y_1), ncol = 1)
    rownames(r) <- names(Y_1)
    colnames(r) <- r_col
    Y_1 <- r
    rm(r, r_col)
  } 
  else {
    Y_1 <- Y_1[base::sort(base::rownames(Y_1)), 
               base::sort(base::colnames(Y_1))]
  }
  if(ncol(Y_2)==1) {
    r_col <- base::colnames(Y_2)
    Y_2 <- Y_2[base::sort(base::rownames(Y_2)),]
    r <- matrix(data = Y_2, nrow = length(Y_2), ncol = 1)
    rownames(r) <- names(Y_2)
    colnames(r) <- r_col
    Y_2 <- r
    rm(r, r_col)
  } 
  else {
    Y_2 <- Y_2[base::sort(base::rownames(Y_2)), 
               base::sort(base::colnames(Y_2))]
  }
  
  N <- base::cbind(base::colSums(Y_1), 
                   base::colSums(Y_2))
  
  return (base::list(Y_1 = Y_1, 
                     Y_2 = Y_2, 
                     N = N, 
                     N_sample = base::ncol(Y_1), 
                     N_gene = base::nrow(Y_1), 
                     gene_names = base::rownames(Y_1),
                     sample_names = base::colnames(Y_1), 
                     proc_ud = Y))
}


u <- get_paired_usage(u = u)
t <- get_paired_usage(u = t)

# add D_test data to object u
u$Y_1_test <- t$Y_1
u$Y_2_test <- t$Y_2
u$N_test <- t$N
u$N_sample_test <- t$N_sample
u$proc_ud_test <- t$proc_ud
rm(t)

# save for later use
save(u, file = "dm_sim/gene_usage_m.RData", compress = T)
```


```{r, echo=FALSE, eval=TRUE}
u <- get(load(file = "dm_sim/gene_usage_m.RData"))
```


```{r, echo = FALSE}
str(u)
```

We can visualize the data using ggplot. Each point is the mRNA count (y-axis)
for a given gene (x-axis) of a sample (point). Notice the differences in 
mRNA counts between the two conditions for genes 12, 5, 7, etc. Filled points
are train and hollow points are test data.

```{r, fig.width=6, fig.height=3.5, fig.align='center', eval=TRUE}
ggplot()+
  geom_point(data = u$proc_ud, aes(x = gene_name, y = gene_usage_count, col = condition),
             position = position_dodge(width = 0.5), size = 1)+
  geom_point(data = u$proc_ud_test, aes(x = gene_name, y = gene_usage_count, col = condition),
             position = position_dodge(width = 0.5), size = 1, shape = 21)+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  scale_color_manual(values = c("steelblue", "orange"))
```

## Modeling
Same models, $DM$ and $M$, as before.

```{r}
fit_dm <- rstan::sampling(object = model_dm,
                          data = u,
                          chains = 5,
                          cores = 5,
                          iter = 3500,
                          warmup = 1500,
                          control = list(adapt_delta = 0.99, max_treedepth = 14),
                          algorithm = "NUTS")
```


```{r, echo = FALSE}
save(fit_dm, file = "dm_sim/fit_dm_data_m.RData", compress = T)
```



```{r}
fit_m <- rstan::sampling(object = model_m,
                         data = u,
                         chains = 5,
                         cores = 5,
                         iter = 3500,
                         warmup = 1500,
                         control = list(adapt_delta = 0.99, max_treedepth = 14),
                         algorithm = "NUTS")
```


```{r, echo = FALSE}
save(fit_m, file = "dm_sim/fit_m_data_m.RData")
```


```{r, echo=FALSE, eval = TRUE, echo=F}
fit_dm <- get(load(file = "dm_sim/fit_dm_data_m.RData"))
fit_m <- get(load(file = "dm_sim/fit_m_data_m.RData"))
```


## Posterior predictive checks for each sample

```{r, eval = TRUE, echo = FALSE}

get_ppc <- function(fit, gene_usage, model_name) {
  summary_y_hat_1 <- data.frame(summary(fit, par = "Y_hat_1")$summary)
  summary_y_hat_1$Y <- as.vector(t(gene_usage$Y_1))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_1|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_1)),
                               split = ','))
  summary_y_hat_1$gene_name <- x[, 1]
  summary_y_hat_1$sample_id <- x[, 2]
  summary_y_hat_1$condition <- "A"
  
  summary_y_hat_2 <- data.frame(summary(fit, par = "Y_hat_2")$summary)
  summary_y_hat_2$Y <-as.vector(t(gene_usage$Y_2))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_2|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_2)),
                               split = ','))
  summary_y_hat_2$gene_name <- x[, 1]
  summary_y_hat_2$sample_id <- x[, 2]
  summary_y_hat_2$condition <- "B"
  
  s <- rbind(summary_y_hat_1, summary_y_hat_2)
  s$model_name <- model_name
  return(s)
}

```

```{r, eval = TRUE, echo = FALSE}
# DM
ppc_dm <- get_ppc(fit = fit_dm, gene_usage = u, model_name = "DM")

# M
ppc_m <- get_ppc(fit = fit_m, gene_usage = u, model_name = "M")

ppc <- rbind(ppc_dm, ppc_m)
rm(ppc_dm, ppc_m)
```

```{r, fig.width=6, fig.height=12, fig.align='center', eval=TRUE}
ggplot(data = ppc)+
  geom_abline(slope = 1, intercept = 0)+
  facet_grid(sample_id~model_name)+
  geom_point(aes(y = mean, x = Y, col = condition), size = 1)+
  geom_errorbar(aes(y = mean, x = Y, ymin = X2.5., ymax = X97.5., col = condition))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Observed Y")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))
```


```{r, eval = TRUE, echo = FALSE}
ppc$in_hdi <- ifelse(ppc$Y >= ppc$X2.5. & ppc$Y <= ppc$X97.5., 
                     yes = "in_hdi", no = "out_hdi")
table(ppc$in_hdi, ppc$model_name)
```


## PPC from condition-specific parameters

```{r, eval=TRUE, echo = FALSE}

get_ppc_group <- function(fit, gene_usage, model_name) {
  summary_y_hat_1 <- data.frame(summary(fit, par = "Y_hat_group_1")$summary)
  summary_y_hat_1$Y_Din <- as.vector(t(gene_usage$Y_1))
  summary_y_hat_1$Y_Dout <- as.vector(t(gene_usage$Y_1_test))
  
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_group_1|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_1)),
                               split = ','))
  summary_y_hat_1$gene_name <- x[, 1]
  summary_y_hat_1$sample_id <- x[, 2]
  summary_y_hat_1$condition <- "A"
  
  summary_y_hat_2 <- data.frame(summary(fit, par = "Y_hat_group_2")$summary)
  summary_y_hat_2$Y_Din <- as.vector(t(gene_usage$Y_2))
  summary_y_hat_2$Y_Dout <- as.vector(t(gene_usage$Y_2_test))
  x <- do.call(rbind, strsplit(x = gsub(pattern = "Y_hat_group_2|\\[|\\]", 
                                        replacement = '', 
                                        x = rownames(summary_y_hat_2)),
                               split = ','))
  summary_y_hat_2$gene_name <- x[, 1]
  summary_y_hat_2$sample_id <- x[, 2]
  summary_y_hat_2$condition <- "B"
  
  s <- rbind(summary_y_hat_1, summary_y_hat_2)
  s$model_name <- model_name
  return(s)
}

```

```{r, eval=TRUE, echo = FALSE}
# DM
ppc_group_dm <- get_ppc_group(fit = fit_dm, gene_usage = u, model_name = "DM")

# M
ppc_group_m <- get_ppc_group(fit = fit_m, gene_usage = u, model_name = "M")

ppc_group <- rbind(ppc_group_dm, ppc_group_m)
rm(ppc_group_dm, ppc_group_m)
```

```{r, fig.width=12, fig.height=7, fig.align='center', eval=TRUE, echo = FALSE}
(ggplot(data = ppc_group)+
  facet_wrap(facets = ~model_name, ncol = 1)+
  geom_point(aes(x = gene_name, y = mean, col = condition), 
                position = position_dodge(width = 0.55),  size = 0.5)+
  geom_point(aes(x = gene_name, y = Y_Din, group = condition),
                position = position_dodge(width = 0.55), size = 1.1)+
  geom_errorbar(aes(x = gene_name, y = mean, ymin = X2.5., ymax = X97.5., col = condition), 
                width = 0.35, position = position_dodge(width = 0.55))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Gene")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))+
   ggtitle(label = "D_train"))|
(ggplot(data = ppc_group)+
  facet_wrap(facets = ~model_name, ncol = 1)+
  geom_point(aes(x = gene_name, y = mean, col = condition), 
                position = position_dodge(width = 0.55),  size = 0.5)+
  geom_point(aes(x = gene_name, y = Y_Dout, group = condition),
                position = position_dodge(width = 0.55), size = 1.1)+
  geom_errorbar(aes(x = gene_name, y = mean, ymin = X2.5., ymax = X97.5., col = condition), 
                width = 0.35, position = position_dodge(width = 0.55))+
  theme_bw(base_size = 10)+
  theme(legend.position = "top")+
  xlab(label = "Gene")+
  ylab(label = "Predicted Y [95% HDI]")+
  scale_color_manual(values = c("steelblue", "orange"))+
   ggtitle(label = "D_test"))
```


### How many $D_{\text{train}}$ observations are outside the 95\% HDIs of the predictions?

```{r, eval=TRUE, echo = FALSE}
ppc_group$in_hdi_in <- ifelse(ppc_group$Y_Din >= ppc_group$X2.5. & 
                             ppc_group$Y_Din <= ppc_group$X97.5., 
                           yes = "in_hdi", no = "out_hdi")
table(ppc_group$in_hdi_in, ppc_group$model_name)
```

### How many $D_{\text{test}}$ observations are outside the 95\% HDIs of the predictions?

```{r, eval=TRUE, echo = FALSE}
ppc_group$in_hdi_out <- ifelse(ppc_group$Y_Dout >= ppc_group$X2.5. & 
                             ppc_group$Y_Dout <= ppc_group$X97.5., 
                           yes = "in_hdi", no = "out_hdi")
table(ppc_group$in_hdi_out, ppc_group$model_name)
```



## Model comparison with the Widely Applicable Information Criterion (WAIC)

### Model $DM$ WAIC

```{r, eval=TRUE, echo = FALSE}
loglik_train_dm <- summary(fit_dm, par = "log_lik_train")$summary
loglik_test_dm <- summary(fit_dm, par = "log_lik_test")$summary
```

#### $D_{\text{train}}$
Notice that the loo package generated similar estimates:

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_train_dm[,"mean"]), "\n")
cat("p_eff:", sum(loglik_train_dm[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_train_dm[,"mean"])-sum(loglik_train_dm[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_train_dm[,"mean"])*sqrt(length(loglik_train_dm[,"mean"])))
# see https://avehtari.github.io/modelselection/CV-FAQ.html#15_Why_(sqrt{n})_in_Standard_error_(SE)_of_LOO
```

#### $D_{\text{test}}$

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_test_dm[,"mean"]), "\n")
cat("p_eff:", sum(loglik_test_dm[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_test_dm[,"mean"])-sum(loglik_test_dm[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_test_dm[,"mean"])*sqrt(length(loglik_test_dm[,"mean"])))
```


### Model $M$ WAIC

```{r, eval=TRUE, echo = FALSE}
loglik_train_m <- summary(fit_m, par = "log_lik_train")$summary
loglik_test_m <- summary(fit_m, par = "log_lik_test")$summary
```

#### $D_{\text{train}}$

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_train_m[,"mean"]), "\n")
cat("p_eff:", sum(loglik_train_m[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_train_m[,"mean"])-sum(loglik_train_m[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_train_m[,"mean"])*sqrt(length(loglik_train_m[,"mean"])))
```

#### $D_{\text{test}}$

```{r, eval=TRUE, echo = FALSE}
cat("lppd:", sum(loglik_test_m[,"mean"]), "\n")
cat("p_eff:", sum(loglik_test_m[,"sd"]^2), "\n")
cat("WAIC:", -2*(sum(loglik_test_m[,"mean"])-sum(loglik_test_m[,"sd"]^2)), "\n")
cat("SE of lppd = lppd * sqrt(N) =:", sd(loglik_test_m[,"mean"])*sqrt(length(loglik_test_m[,"mean"])))
```





